---
title: 'Data 605 Final Project - Part 1'
Author: Charls Joseph
---

### Youtube Video Link
https://www.youtube.com/watch?v=AkDOaz5iAWs&feature=youtu.be


```{r}
set.seed(123)
n = 100
X <- runif(10000, min = 6, max = n)

mean = (n + 1 )/2
sd = (n + 1 )/2

Y <- rnorm(10000, sd= sd , mean= mean)

summary(Y)

summary(X)




```

```{r}
# 'x' is median of X variable
x <- median(X)
 x 
# 'y' is 1st quartile of Y variable
y <- quantile(Y)[2]
y


```


### Qn1 : Calculate as a minimum the below probabilities a through c.  Assume the small letter "x" is estimated as the median of the X variable, and the small letter "y" is estimated as the 1st quartile of the Y variable.  Interpret the meaning of all probabilities.

##  a.   P(X>x | X>y)		b.  P(X>x, Y>y)		c.  P(X<x | X>y)		

```{r}

df <- as.data.frame(X)
df$Y <- Y 

##P(X>x | X>y) is denoted as P_a

#P(A | B ) = P(A and B )/P(B)

Prob_A_and_B <- NROW(df[df$X > x & df$X >y,])/NROW(df)
Prob_B <- NROW(df[df$X >y,])/NROW(df)
P_a <- Prob_A_and_B/Prob_B
P_a

## P(X>x, Y>y) 

P_b <- NROW(df[df$X > x & df$Y >y,])/NROW(df)
P_b

##  P(X<x | X>y) is denoted as P_c

#P(C | B ) = P(C and B )/PB
Prob_C_and_B = NROW(df[df$X < x & df$X >y,])/NROW(df)
P_c <- Prob_C_and_B/Prob_B
P_c


```

### Qn 2: Investigate whether P(X>x and Y>y)=P(X>x)P(Y>y) by building a table and evaluating the marginal and joint probabilities.

Below is the code to generate the marginal and joint probabilites from randomly generated X, Y data. 

```{r}

cell_0_0 <- NROW(df[df$X > x & df$Y > y,])
cell_0_1 <- NROW(df[df$X < x & df$Y > y,])
cell_0_2 <- NROW(df[df$X == x & df$Y > y,])

cell_1_0 <- NROW(df[df$X > x & df$Y < y,])
cell_1_1 <- NROW(df[df$X < x & df$Y < y,])
cell_1_2 <- NROW(df[df$X == x & df$Y < y,])

cell_2_0 <- NROW(df[df$X > x & df$Y == y,])
cell_2_1 <- NROW(df[df$X < x & df$Y == y,])
cell_2_2 <- NROW(df[df$X == x & df$Y == y,])

t <- c(cell_0_0, cell_0_1, cell_0_2)
t <- rbind(t,c(cell_1_0, cell_1_1, cell_1_2))
t <- rbind(t,c(cell_2_0, cell_2_1, cell_2_2))
t <- cbind(t, t[,1] + t[,2] + t[,3])
t <- rbind(t, t[1,] + t[2,] + t[3,])

colnames(t) <- c('X>x', 'X<x', 'X=x', 'Total')
rownames(t) <- c('Y>y', 'Y<y', 'Y=y', 'Total')

t


# P(X>x and Y>y)=P(X>x)P(Y>y) 

```

Now, building a anonymous function to determine the probabilities and apply that fn to all elem in the dataframe. 

```{r}

func = function(x) x/10000
p <- apply(t, MARGIN = c(1,2), FUN = func)
p
# P(X>x and Y>y)=P(X>x)P(Y>y) is denoted as P(A and B) = P(A) * P(B)
```

According to the above contigency probabily table, we calculate the probability of P(A and B), P(A) and P(B), prooved that P(A and B) is equal to the P(A) * P(B)

```{r}
P_A_B <- 0.3756
P_A <- 0.5000
P_B <- 0.75

P_AB <- P_A * P_B 
P_AB
P_A_B

```

### Qn 3 Check to see if independence holds by using Fisher's Exact Test and the Chi Square Test.  What is the difference between the two? Which is most appropriate?

Lets feed the contigency table into chisq test api. The contigency table only contains the joint proportions. 
You have 2 hypothesis. 
1. Null hypothesis -> X, Y are independent 
2. Alternate Hypothesis -> X, Y are dependent. 

```{r}
mat <- matrix(c(3756, 3744, 1244,  1256), 2, 2, byrow=T) 
chi <- chisq.test(mat, correct = TRUE)

chi

```

Since the p-value is greater than 0.05, we cant reject null hypothesis that means X, Y are independent. 
now, lets Fischer test to find the p-value. 

```{r}
fis <- fisher.test(mat, conf.int = T , conf.level = 0.95)
fis
```

In Fischer test, p-value comes as 0.7995 which is same as Chi Sqaure P-value. Some of the Chi Square and Fischer test are given below. 

1. Fischer test is most suited for small samples or proportions. Chi Square is suited for large samples. 
2. Fischer test is highly computational. This is the best hyposthesis testing and gives correct p-value in proportion is small or large. 


